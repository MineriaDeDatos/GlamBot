import requests
import pyttsx3
import speech_recognition as sr
import json

# Direcci√≥n del servidor local (aseg√∫rate de que LM Studio est√© corriendo con el modelo)
SERVER_URL = "http://192.168.1.88:1234/v1/chat/completions"
USER_DATA_URL = "http://192.168.1.88:5000/get_combined_data"  # URL para obtener los datos combinados

# Definir variables para el modelo
NOMBRE_MODELO = "Meta-Llama-3.1-8B-Instruct"  # Nombre del modelo en LM Studio

# Inicializaci√≥n de la voz
engine = pyttsx3.init()

# Configuraci√≥n de voz personalizada
VOZ_OBJETIVO = "HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_ES-MX_SABINA_11.0"

# Buscar la voz exacta
voz_encontrada = None
for voz in engine.getProperty('voices'):
    if voz.id == VOZ_OBJETIVO:
        voz_encontrada = voz
        break

if voz_encontrada:
    engine.setProperty('voice', voz_encontrada.id)
    print(f"‚úÖ Voz seleccionada: {voz_encontrada.name}")
else:
    print("‚ö†Ô∏è Voz preferida no encontrada. Usando voz por defecto.")
    print(f"Voces disponibles: {[voz.name for voz in engine.getProperty('voices')]}")

# Configuraci√≥n de la velocidad de habla y volumen
engine.setProperty("rate", 160)  # Velocidad de habla
engine.setProperty("volume", 0.9)  # Volumen


# Funci√≥n para limpiar el texto (eliminar caracteres innecesarios como # y *)
def clean_text(text):
    return text.replace("#", "").replace("*", "").replace("**", "").strip()


# Funci√≥n para convertir el texto en voz
def speak(text):
    text = clean_text(text)  # Limpiar el texto antes de hablar
    engine.say(text)
    engine.runAndWait()


# Funci√≥n para capturar la entrada de voz del usuario
def listen():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        try:
            recognizer.adjust_for_ambient_noise(source, duration=0.8)
            print("\nüé§ [Escuchando...]")
            audio = recognizer.listen(source, timeout=12, phrase_time_limit=18)
            text = recognizer.recognize_google(audio, language="es-ES")
            print(f"üë§ [Usuario]: {text}")
            return text
        except sr.WaitTimeoutError:
            print("‚è≥ [Sistema]: Tiempo de espera agotado")
            return None
        except Exception as e:
            print(f"‚ùå [Error]: {str(e)}")
            return None


# Funci√≥n para generar respuesta con el modelo local (LM Studio)
def generate_response_with_local_model(user_input, features, skin_type, name, conversation_history):
    try:
        # Convertir las caracter√≠sticas a una descripci√≥n en texto
        descripcion_features = (
            f"Tengo un rostro {features['rostro']}, ojos {features['ojos']} y piel {features['tono_piel']}. "
            f"Mi tipo de piel es {skin_type}."
        )

        # Mantener el historial de conversaci√≥n
        conversation_history.append({"role": "user", "content": user_input})

        # Datos de entrada para el modelo, incluyendo el historial de la conversaci√≥n
        payload = {
            "model": NOMBRE_MODELO,
            "messages": [{"role": "system", "content": f"Eres un experto en maquillaje y belleza personal. "
                                                       f"Siempre me llamas '{name}'. Mi informaci√≥n de belleza es la siguiente: {descripcion_features}"},
                         *conversation_history],
            "temperature": 0.7,  # Nivel de creatividad del modelo
            "max_tokens": 300  # L√≠mite de tokens generados
        }

        # Realizar la petici√≥n POST al servidor
        response = requests.post(SERVER_URL, json=payload)

        # Si la respuesta es exitosa, procesarla
        if response.status_code == 200:
            response_data = response.json()
            answer = response_data.get('choices', [{}])[0].get('message', {}).get('content', '')

            # Si la respuesta no es sobre maquillaje, redirigir al tema correcto
            if "no relacionado" in answer.lower() or "pregunta fuera de tema" in answer.lower():
                return f"{name}, recuerda que solo hablo sobre maquillaje y belleza. ¬øQuieres consejos de maquillaje?"

            return answer
        else:
            print(f"‚ö†Ô∏è Error al obtener la respuesta del modelo: {response.status_code}")
            return "Ocurri√≥ un error al procesar tu solicitud."

    except Exception as e:
        print(f"‚ö†Ô∏è Error al realizar la solicitud: {e}")
        return "Ocurri√≥ un error procesando tu solicitud."


# Funci√≥n para obtener los datos del usuario desde el servidor
def get_user_features():
    try:
        response = requests.get(USER_DATA_URL)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"‚ö†Ô∏è Error al obtener los datos del usuario: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error al hacer la solicitud GET: {e}")
        return None


# Funci√≥n para preguntar preferencias al usuario
def preguntar_preferencias(name):
    speak(f"{name}, ¬øqu√© tipo de maquillaje prefieres hoy?")
    print(f"ü§ñ [GlamBot]: {name}, ¬øqu√© tipo de maquillaje prefieres hoy?")
    preferencia = listen()

    if not preferencia:
        return "natural"  # Valor por defecto si no responde

    return preferencia


# Funci√≥n para generar el prompt final de maquillaje basado en toda la conversaci√≥n
def generate_makeup_prompt(conversation_history):
    prompt = "Generar maquillaje basado en la siguiente conversaci√≥n:\n"

    for message in conversation_history:
        if message["role"] == "user":
            prompt += f"Usuario: {message['content']}\n"
        elif message["role"] == "assistant":
            prompt += f"Asistente: {message['content']}\n"

    return prompt


# Funci√≥n principal para interactuar con el asistente
def interact_with_assistant():
    # Obtener los datos de morfolog√≠a del usuario
    user_features = get_user_features()

    if not user_features:
        speak("Lo siento, no pude obtener los datos de morfolog√≠a. ¬øPuedes intentar de nuevo?")
        print("‚ö†Ô∏è [Sistema]: No se pudieron obtener los datos del usuario.")
        return

    # Extraer el nombre y las caracter√≠sticas del usuario
    name = user_features.get("name", "Usuario")
    features = user_features.get("features", {})
    skin_type = user_features.get("skin_type", "Desconocido")

    print(f"ü§ñ [GlamBot]: Hola {name}, soy GlamBot. ¬øEn qu√© puedo ayudarte hoy?")
    speak(f"Hola {name}, soy GlamBot. ¬øEn qu√© puedo ayudarte hoy?")

    # Crear historial de conversaci√≥n vac√≠o
    conversation_history = []

    while True:
        user_input = listen()

        if not user_input:
            continue

        # Verificar si el usuario quiere salir
        if any(word in user_input.lower() for word in ["salir", "terminar", "adi√≥s"]):
            farewell = f"¬°Gracias por conversar, {name}! Hasta la pr√≥xima."
            print(f"ü§ñ [GlamBot]: {farewell}")
            speak(farewell)

            # Generar el prompt final basado en la conversaci√≥n
            final_prompt = generate_makeup_prompt(conversation_history)
            print(f"üåü [Generado]: {final_prompt}")

            # Aqu√≠ puedes almacenar el prompt o hacer lo que desees con √©l
            # Por ejemplo, guardarlo en un archivo
            with open("final_prompt.txt", "w") as f:
                f.write(final_prompt)

            break

        # Preguntar preferencias usando el nombre del usuario
        preferencia_maquillaje = preguntar_preferencias(name)

        # Generar respuesta con el modelo incluyendo las preferencias y datos de caracter√≠sticas
        input_con_preferencia = f"{user_input}. Me gustar√≠a un maquillaje {preferencia_maquillaje}."
        response = generate_response_with_local_model(input_con_preferencia, features, skin_type, name,
                                                      conversation_history)

        print(f"ü§ñ [GlamBot]: {response}")
        speak(response)

        # Agregar la respuesta del asistente al historial
        conversation_history.append({"role": "assistant", "content": response})


# Ejecutar el asistente
if __name__ == "__main__":
    interact_with_assistant()



-------------------------------------------------------------------------------------------------

import requests
import pyttsx3
import speech_recognition as sr
import json
import base64
from flask import Flask, request, jsonify
from io import BytesIO

# Configuraci√≥n del servidor y el modelo
SERVER_URL = "http://192.168.1.88:1234/v1/chat/completions"
USER_DATA_URL = "http://192.168.1.88:5000/get_combined_data"  # URL para obtener los datos combinados
NOMBRE_MODELO = "Meta-Llama-3.1-8B-Instruct"  # Nombre del modelo en LM Studio

# Inicializaci√≥n de la voz
engine = pyttsx3.init()
engine.setProperty("rate", 160)  # Velocidad de habla
engine.setProperty("volume", 0.9)  # Volumen
VOZ_OBJETIVO = "HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_ES-MX_SABINA_11.0"

# Configuraci√≥n de la voz
voz_encontrada = None
for voz in engine.getProperty('voices'):
    if voz.id == VOZ_OBJETIVO:
        voz_encontrada = voz
        break
if voz_encontrada:
    engine.setProperty('voice', voz_encontrada.id)

# Flask para manejar interacciones
app = Flask(__name__)


# Funci√≥n para obtener los datos del usuario desde el servidor
def get_user_features():
    try:
        response = requests.get(USER_DATA_URL)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"‚ö†Ô∏è Error al obtener los datos del usuario: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error al hacer la solicitud GET: {e}")
        return None


# Funci√≥n para limpiar el texto (eliminar caracteres innecesarios)
def clean_text(text):
    return text.replace("#", "").replace("*", "").replace("**", "").strip()

def generate_response_with_local_model(user_input, features, skin_type, name, conversation_history):
    try:
        user_input = clean_text(user_input)

        # Descripci√≥n de las caracter√≠sticas
        descripcion_features = (
            f"Tengo un rostro {features['rostro']}, ojos {features['ojos']} y piel {features['tono_piel']}. "
            f"Mi tipo de piel es {skin_type}."
        )

        conversation_history.append({"role": "user", "content": user_input})

        payload = {
            "model": NOMBRE_MODELO,
            "messages": [{"role": "system",
                          "content": f"Eres un experto en maquillaje y belleza personal. Siempre me llamas 'GlamBot'. Mi informaci√≥n de belleza es la siguiente: {descripcion_features}"},
                         *conversation_history],
            "temperature": 0.7,
            "max_tokens": 300
        }

        # Llamada al modelo
        response = requests.post(SERVER_URL, json=payload)
        if response.status_code == 200:
            response_data = response.json()
            generated_content = response_data.get('choices', [{}])[0].get('message', {}).get('content', '')

            # Regresar tanto la respuesta como el prompt generado
            return generated_content, payload["messages"]
        else:
            return "Error al procesar la solicitud", None
    except Exception as e:
        return f"Error: {e}", None

# Funci√≥n para convertir texto en voz y devolverlo como base64
def generate_audio_response(text):
    engine.save_to_file(text, "response.mp3")
    engine.runAndWait()

    # Convertir audio a base64
    with open("response.mp3", "rb") as audio_file:
        audio_base64 = base64.b64encode(audio_file.read()).decode('utf-8')

    return audio_base64


# Funci√≥n para capturar la entrada de voz del usuario (escucha)
def listen():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        try:
            recognizer.adjust_for_ambient_noise(source, duration=0.8)
            print("\nüé§ [Escuchando...]")
            audio = recognizer.listen(source, timeout=12, phrase_time_limit=18)
            text = recognizer.recognize_google(audio, language="es-ES")
            print(f"üë§ [Usuario]: {text}")
            return clean_text(text)  # Limpiar el texto antes de devolverlo
        except sr.WaitTimeoutError:
            print("‚è≥ [Sistema]: Tiempo de espera agotado")
            return None
        except Exception as e:
            print(f"‚ùå [Error]: {str(e)}")
            return None


# Funci√≥n principal para interactuar con el asistente
def interact_with_assistant(user_input, features, skin_type, name, conversation_history):
    if not user_input:
        return "Error: No input received"

    # Aqu√≠ cambiamos la presentaci√≥n para que siempre se identifique como "GlamBot"
    response = generate_response_with_local_model(user_input, features, skin_type, "GlamBot", conversation_history)

    # Agregar la respuesta del asistente al historial de conversaci√≥n
    conversation_history.append({"role": "assistant", "content": response})

    return response


# Ruta para recibir mensajes del cliente y generar la respuesta
@app.route("/interact", methods=["POST"])
def interact():
    # Obtener los datos del usuario
    user_features = get_user_features()
    if user_features is None:
        return jsonify({"error": "No se pudieron obtener los datos del usuario"}), 500

    data = request.json
    user_input = data.get('message')
    features = user_features.get("features", {})
    skin_type = user_features.get("skin_type", "desconocido")
    name = user_features.get("name", "Usuario")
    conversation_history = data.get('conversation_history', [])

    # Limpiar el texto del usuario antes de procesarlo
    user_input = clean_text(user_input)

    # Obtener la respuesta del modelo
    response_text = interact_with_assistant(user_input, features, skin_type, name, conversation_history)

    # Generar audio para la respuesta
    audio_base64 = generate_audio_response(response_text)

    return jsonify({
        "text_response": response_text,
        "audio_response": audio_base64
    })


# Ejecutar el servidor Flask
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)